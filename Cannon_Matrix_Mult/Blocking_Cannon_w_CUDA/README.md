# Parallel Matrix Multiplication using Cannonâ€™s Algorithm (CUDA + MPI)

This project implements a **hybrid MPI + CUDA** version of Cannonâ€™s algorithm for dense matrix multiplication, evaluated on the **Leonardo Booster** partition at CINECA.

---

## ğŸ§© Overview

Cannonâ€™s algorithm decomposes a square matrix multiplication  
\[
C = A \times B
\]
across an \( N \times N \) MPI process grid. Each MPI rank performs local matrix blocks multiplication and communicates sub-blocks with its neighbors in a toroidal topology.

This implementation offloads each local block multiplication to **NVIDIA A100 GPUs** using CUDA kernels and coordinates inter-process communication with **MPI**.

---

## âš™ï¸ Code Structure

```
.
â”œâ”€â”€ main.cu                     # Main Cannonâ€™s algorithm (MPI + CUDA)
â”œâ”€â”€ Parallel_CSimple_timer.hpp  # Timing utility (MPI-aware)
â”œâ”€â”€ test.cpp                    # Validation test for correctness
â”œâ”€â”€ compile.job                 # Build script for Leonardo
â”œâ”€â”€ 1node.job                   # Run script (Slurm batch)
â”œâ”€â”€ plot.gp                     # Gnuplot script for performance plot
â””â”€â”€ flops.png                   # Final performance plot
```

---

## ğŸš€ Compilation

Compilation is automated by **`compile.job`**, which loads the proper modules and builds `main.cu` with NVHPC or CUDA + MPI toolchains.  
For manual compilation:

```bash
mpicxx -O3 -std=c++17 -ccbin nvc++ main.cu -o cannon_gpu.x        -I. -lcudart -L${CUDA_HOME}/lib64
```

---

## ğŸ§  Execution on Leonardo

The provided **`1node.job`** script runs the executable on one Booster node:

```bash
sbatch 1node.job
```

### Leonardo Booster hardware
> **Booster partition (BullSequana X2135 â€œDa Vinciâ€):**  
> 3456 nodes, each with a single-socket 32-core **Intel Xeon Platinum 8358** CPU (2.60 GHz),  
> 512 GB DDR4 RAM, **4 Ã— NVIDIA A100 GPUs** (64 GB HBM2e, NVLink 3.0, 200 GB/s),  
> interconnect: **2 Ã— dual-port HDR100** (400 Gb/s per node).

Each rankâ€“GPU pair executes a local matrix block multiplication, and timing is collected via `Parallel_CSimple_timer.hpp` using synchronized MPI barriers to ensure consistent measurements.

---

## ğŸ“Š Numerical Experiment

The figure below summarizes the measured performance:

![Compute Intensity of Blocking Cannon](flops.png)

**Figure 1.** Compute intensity (TeraFLOP/s) of Cannonâ€™s algorithm with 4 GPUs Ã— 4 MPI processes on Leonardo Booster.  
Each data point represents the average sustained performance across multiple runs for increasing matrix sizes.

---

## ğŸ” Observations

- **Scalability:** Performance increases nearly linearly with matrix size until GPU memory saturation, reflecting efficient overlap of communication and computation.  
- **Peak Performance:** The implementation achieves â‰ˆ 24 TeraFLOP/s on the largest tested matrix, consistent with expected A100 GPU peak performance for double-precision GEMM when distributed across 4 GPUs.  
- **Communication Overhead:** For smaller matrices, performance drops due to startup latency in MPI broadcasts and synchronization phases.  
- **Load Balance:** Uniform block decomposition ensures near-perfect workload distribution across ranks.

---

## ğŸ“ˆ Visualization

Recreate the performance plot with:

```bash
gnuplot plot.gp
```

This produces `flops.png` using data from `times.dat`.

---

## ğŸ§¾ References

- Cannon, L.E. *â€œA Cellular Computer to Implement the Kalman Filter Algorithm.â€* PhD Thesis, Montana State University (1969).  
- NVIDIA Developer Blog â€“ *â€œScaling Matrix Multiplication across GPUs with CUDA and MPI.â€*

---

## ğŸ§° Acknowledgments

Experiments executed on the **Leonardo Booster** partition at **CINECA** under the ICTPâ€“MHPC program.  
Special thanks to the CINECA support team for maintaining the NVHPC and HPC-X environments.

---
